{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "b731dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "09e31cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('datasets/question-3-features-train.csv')\n",
    "Y = pd.read_csv('datasets/question-3-labels-train.csv')\n",
    "test_X = pd.read_csv('datasets/question-3-features-test.csv')\n",
    "test_Y = pd.read_csv('datasets/question-3-labels-test.csv')\n",
    "\n",
    "X = X.to_numpy()\n",
    "Y = Y.to_numpy()\n",
    "X = (X - X.mean()) / (X.max() - X.min()) ## min-max normalization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227639e9",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "a5af5764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(gt_y, pred_y):\n",
    "    # initializing to 1 in order to avoid division by 0\n",
    "    tp = 0.1\n",
    "    tn = 0.1\n",
    "    fp = 0.1\n",
    "    fn = 0.1\n",
    "    for g_y, p_y in zip(gt_y, pred_y):\n",
    "        if g_y == 0 and p_y == 0:\n",
    "            tn += 1\n",
    "        elif g_y == 1 and p_y == 1:\n",
    "            tp += 1\n",
    "        elif g_y == 1 and p_y == 0:\n",
    "            fn += 1\n",
    "        elif g_y == 0 and p_y == 1:\n",
    "            fp += 1\n",
    "    precision = tp / tp + fp\n",
    "    recall = tp / tp + fn\n",
    "    npv = tn / fn + tn\n",
    "    fpr = tn / fp + tn\n",
    "    fdr = fp / fp + tp\n",
    "    f1 = (2 * recall * precision) / (recall + precision)\n",
    "    f2 = (5 * precision * recall) / (4 * precision + recall)\n",
    "    return precision,recall,npv,fpr,fdr,f1,f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "9f13bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(gt_y, pred_y):\n",
    "    correct = 0\n",
    "    for g_y, p_y in zip(gt_y, pred_y):\n",
    "        if g_y == p_y:\n",
    "            correct += 1\n",
    "    return (correct/float(len(gt_y)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "cd333636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(scores):\n",
    "    return 1 / (1 + np.exp(-scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "4673fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(features, target, weights):\n",
    "    scores = np.dot(features, weights)\n",
    "    ll = np.sum( target*scores - np.log(1 + np.exp(scores)) )\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "0b8f59d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(features, target, num_steps, learning_rate, add_intercept = False):\n",
    "    if add_intercept:\n",
    "        intercept = np.ones((features.shape[0], 1))\n",
    "        features = np.hstack((intercept, features))\n",
    "        \n",
    "    weights = np.zeros(features.shape[1])\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        scores = np.dot(features, weights)\n",
    "        predictions = sigmoid(scores)\n",
    "\n",
    "        # Update weights with gradient\n",
    "        output_error_signal = target - predictions\n",
    "        gradient = np.dot(features.T, output_error_signal)\n",
    "        weights += learning_rate * gradient\n",
    "        \n",
    "        # Print log-likelihood every so often\n",
    "        #if step % 10000 == 0:\n",
    "        #    print(log_likelihood(features, target, weights))\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "f0b44b9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-630-8d36540f30b6>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-scores))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.1    Accuracy : 69.27374301675978\n",
      "Precision: 40.1 Recall: 48.1 NPV: 75.6732484076433 FPR: 75.99514066496162 FDR: 20.1\n",
      "F1: 43.737188208616786 F2: 46.254436450839336\n",
      "\n",
      "Learning Rate: 0.01    Accuracy : 72.06703910614524\n",
      "Precision: 26.1 Recall: 53.1 NPV: 89.79097888675624 FPR: 91.60996015936254 FDR: 15.1\n",
      "F1: 34.997727272727275 F2: 43.99714285714286\n",
      "\n",
      "Learning Rate: 0.001    Accuracy : 70.39106145251397\n",
      "Precision: 20.1 Recall: 56.1 NPV: 95.80780399274046 FPR: 99.02670157068061 FDR: 12.1\n",
      "F1: 29.596062992125987 F2: 41.30439560439561\n",
      "\n",
      "Learning Rate: 0.0001    Accuracy : 72.06703910614524\n",
      "Precision: 23.1 Recall: 54.1 NPV: 92.8156308851224 FPR: 95.22217194570135 FDR: 14.1\n",
      "F1: 32.37590673575129 F2: 42.65221843003413\n",
      "\n",
      "Learning Rate: 1e-05    Accuracy : 68.71508379888269\n",
      "Precision: 33.1 Recall: 52.1 NPV: 82.68708414872798 FPR: 83.62647975077881 FDR: 16.1\n",
      "F1: 40.48145539906103 F2: 46.73468834688347\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y = np.squeeze(Y)\n",
    "rates = [1e-01,1e-02,1e-03,1e-04,1e-05]\n",
    "for current_rate in rates:\n",
    "    # training the model to obtain the weights\n",
    "    weights = logistic_regression(X, Y, num_steps = 1000, learning_rate = current_rate, add_intercept=True)\n",
    "    # preparing test data\n",
    "    data_with_intercept = np.hstack((np.ones((test_X.shape[0], 1)), test_X))\n",
    "    # multiply test data with our weights\n",
    "    final_scores = np.dot(data_with_intercept, weights)\n",
    "    # prediction with test data and trained weights\n",
    "    preds = np.round(sigmoid(final_scores))\n",
    "    report = confusion_matrix(Y,preds)\n",
    "    print('Learning Rate: ' + str(current_rate) + '    Accuracy : ' + str(calc_accuracy(test_Y['Survived'], preds)))    \n",
    "    print('Precision: ' + str(report[0]) + ' Recall: ' + str(report[1]) + ' NPV: ' + str(report[2]) + \n",
    "          ' FPR: ' + str(report[3]) + ' FDR: ' + str(report[4]) + '\\nF1: ' + str(report[5]) + ' F2: ' + str(report[6]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7616710",
   "metadata": {},
   "source": [
    "### Learning rate 0.01 gives %72 accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e3fd4",
   "metadata": {},
   "source": [
    "##### Ignore the overflow encountered error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a364b801",
   "metadata": {},
   "source": [
    "### Mini-Batch Stochastic Gradient Ascent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "id": "de50b094",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.657303</td>\n",
       "      <td>0.516361</td>\n",
       "      <td>0.005325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.342697</td>\n",
       "      <td>-0.174767</td>\n",
       "      <td>-0.046603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.342697</td>\n",
       "      <td>-0.049108</td>\n",
       "      <td>-0.046904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.342697</td>\n",
       "      <td>0.001156</td>\n",
       "      <td>-0.048205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.342697</td>\n",
       "      <td>-0.002624</td>\n",
       "      <td>-0.046603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>-0.657303</td>\n",
       "      <td>-0.137069</td>\n",
       "      <td>0.115459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>-0.657303</td>\n",
       "      <td>0.302740</td>\n",
       "      <td>0.053626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>0.342697</td>\n",
       "      <td>-0.137069</td>\n",
       "      <td>-0.046603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>-0.657303</td>\n",
       "      <td>0.026288</td>\n",
       "      <td>-0.002783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>0.342697</td>\n",
       "      <td>0.114250</td>\n",
       "      <td>-0.046847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>712 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pclass       Age      Fare\n",
       "0   -0.657303  0.516361  0.005325\n",
       "1    0.342697 -0.174767 -0.046603\n",
       "2    0.342697 -0.049108 -0.046904\n",
       "3    0.342697  0.001156 -0.048205\n",
       "4    0.342697 -0.002624 -0.046603\n",
       "..        ...       ...       ...\n",
       "707 -0.657303 -0.137069  0.115459\n",
       "708 -0.657303  0.302740  0.053626\n",
       "709  0.342697 -0.137069 -0.046603\n",
       "710 -0.657303  0.026288 -0.002783\n",
       "711  0.342697  0.114250 -0.046847\n",
       "\n",
       "[712 rows x 3 columns]"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv('datasets/question-3-features-train.csv')\n",
    "Y = pd.read_csv('datasets/question-3-labels-train.csv')\n",
    "test_X = pd.read_csv('datasets/question-3-features-test.csv')\n",
    "test_Y = pd.read_csv('datasets/question-3-labels-test.csv')\n",
    "X = (X - X.mean()) / (X.max() - X.min()) ## min-max normalization \n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "4151f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_ascent_regression(X,learning_rate=0.01, n_epochs=1000, k=100):  \n",
    "    # initialising weights to gaussian random numbers N(0,0.01)\n",
    "    w = random.normal(loc=0, scale=0.01, size = X.shape[1] - 1)\n",
    "    #print(w.shape)\n",
    "    b = np.random.randn(1,1)   # Random intercept value\n",
    "    epoch = 1\n",
    "    while epoch <= n_epochs:\n",
    "        # taking mini batches with size k\n",
    "        temp = X.sample(k)\n",
    "        X_tr = temp.iloc[:,:3].values\n",
    "        #print(X_tr)\n",
    "        #print(X_tr.shape)\n",
    "        y_tr = temp.iloc[:,-1].values\n",
    "        #print(y_tr)\n",
    "        #print(y_tr.shape)\n",
    "        Lw = w\n",
    "        Lb = b\n",
    "        for i in range(k):\n",
    "            # calculate derivative of gradients\n",
    "            Lw = (-2/k * X_tr[i]) * (y_tr[i] - np.dot(X_tr[i],w.T) - b)\n",
    "            Lb = (-2/k) * (y_tr[i] - np.dot(X_tr[i],w.T) - b)\n",
    "            # gradient ascent\n",
    "            w = w + learning_rate * Lw\n",
    "            b = b + learning_rate * Lb\n",
    "        epoch += 1\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "aa3b1e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01    Accuracy : 55.3072625698324\n",
      "Precision: 1.1 Recall: 1.1 NPV: 1.1 FPR: 1.1 FDR: 1.1\n",
      "F1: 1.1 F2: 1.1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-630-8d36540f30b6>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-scores))\n"
     ]
    }
   ],
   "source": [
    "weights_sga = stochastic_gradient_ascent_regression(X.join(Y['Survived']))\n",
    "final_scores = np.dot(X, weights_sga.squeeze())\n",
    "preds = np.round(sigmoid(final_scores))\n",
    "report = confusion_matrix(test_Y,preds)\n",
    "print('Learning Rate: ' + str(0.01) + '    Accuracy : ' + str(calc_accuracy(test_Y['Survived'], preds)))\n",
    "print('Precision: ' + str(report[0]) + ' Recall: ' + str(report[1]) + ' NPV: ' + str(report[2]) + \n",
    "     ' FPR: ' + str(report[3]) + ' FDR: ' + str(report[4]) + '\\nF1: ' + str(report[5]) + ' F2: ' + str(report[6]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e60cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
